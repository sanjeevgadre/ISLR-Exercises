---
title: "Question 9 - Solutions"
author: "Sanjeev Gadre"
date: "October 20, 2018"
output: md_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
library(pls)
library(ISLR)
data("College")
```

```{r functions, include=FALSE, message=FALSE}

```

##9.a
Splitting the data into a training set and a test set in the ratio of 75:25
```{r 9-a}
set.seed(1)
train = sample(c(TRUE, FALSE), nrow(College), replace = TRUE, prob = c(0.75,0.25))
test = !(train)

#Total squared error if the response variable of all test observations were predicted to have the average value of the response variable for all training observations.

TSS.test = sum((mean(College$Apps[train])-College$Apps[test])^2)
```

##9.b
Least square regression model
```{r 8-b}
lm.fit = lm(Apps~., data = College, subset = train)
pred.lm.fit = predict(lm.fit, newdata = College[test,], interval = "prediction", type = "response")
RSS.lm.fit = sum((pred.lm.fit-College$Apps[test])^2)
R2.lm.fit = 1-(RSS.lm.fit/TSS.test)
R2.lm.fit
```

##9.c
```{r 9-c}
x.matrix = model.matrix(Apps~., data = College)[,-1] #the [,-1] gets rid of the                                                          intercept term
y = College$Apps

set.seed(11)
ridge.fit = glmnet(x.matrix[train,], y[train], alpha = 0)
cv.ridge.fit = cv.glmnet(x.matrix[train,], y[train], alpha = 0)
best.lambda.ridge = cv.ridge.fit$lambda.min

pred.ridge.fit = predict(ridge.fit, newx = x.matrix[test,], s = best.lambda.ridge)
RSS.ridge.fit = sum((pred.ridge.fit-y[test])^2)
R2.ridge.fit = 1-(RSS.ridge.fit/TSS.test)
R2.ridge.fit
```

###9.d
```{r 9-d}
set.seed(21)
lasso.fit = glmnet(x.matrix[train,], y[train], alpha = 1)
cv.lasso.fit = cv.glmnet(x.matrix[train,], y[train], alpha =1)
best.lambda.lasso = cv.lasso.fit$lambda.min

pred.lasso.fit = predict(lasso.fit, newx = x.matrix[test,], s = best.lambda.lasso)
RSS.lasso.fit = sum((pred.lasso.fit-y[test])^2)
R2.lasso.fit = 1-(RSS.lasso.fit/TSS.test)
R2.lasso.fit
```

###9.e
```{r 9-e}
set.seed(31)
pcr.fit = pcr(Apps~., data = College, subset = train, scale = TRUE, validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")

#From the graph I am inferring that the lowest CV error occurs for M=5, after which it starts to flatten out. This is the value of M I will use for prediction.
pred.pcr.fit = predict(pcr.fit, newdata = College[test,], ncomp = 5)
RSS.pcr.fit = sum((pred.pcr.fit-y[test])^2)
R2.pcr.fit = 1-(RSS.pcr.fit/TSS.test)
R2.pcr.fit
```

###9.f
```{r 9-f}
set.seed(41)
pls.fit = plsr(Apps~., data = College, subset = train, scale = TRUE, validation = "CV")
validationplot(pls.fit, val.type = "MSEP")

#From the graph I am inferring that the lowest CV error occurs for M=3, after which it starts to flatten out. This is the value of M I will use for prediction.
pred.pls.fit = predict(pls.fit, newdata = College[test,], ncomp = 3)
RSS.pls.fit = sum((pred.pls.fit-y[test])^2)
R2.pls.fit = 1-(RSS.pls.fit/TSS.test)
R2.pls.fit
```

###9.g
The table below presents the summary of the performance of the 5 models using R^2 as a measure of comparison. The Lasso model outperforms all the other models significantly, though the LSE model is greatly improved upon by all the 4 models. 
```{r 9-g-i}
matrix(data = c("Least Square", "Ridge", "Lasso", "Principal Component", "Partial Least Squares", R2.lm.fit, R2.ridge.fit, R2.lasso.fit, R2.pcr.fit, R2.pls.fit), ncol = 2, dimnames = list(NULL, c("Model Type", "R2")))
```

The table below presents the lasso regression coeffiecients for the predictor variables, for the "best" value of lambda and compares them with the least square regression coeffiecients. 

For an apples-to-apples comparison we have used only the training set in calculating these coeffiecients. In this case while Lasso does significantly outperform Least Square, it improves the interpretability of the model only marginally, as Lasso is able to drive the coefficients of only 3 variables (Enroll, Books and F.Undergrad) to zero.

```{r 9-g-ii}
#Deriving the coefficients of best lambda
lasso.coef = predict(lasso.fit, type = "coefficients", s = best.lambda.lasso)[1:18]

cbind(Predictor = c("Intercept",names(College[-2])), Lasso_Coeffs = lasso.coef, Least_Square_Coeffs = as.numeric(lm.fit$coefficients))
```

