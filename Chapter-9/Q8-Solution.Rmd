---
title: "Q8-Solution"
author: "Sanjeev Gadre"
date: "January 10, 2019"
output: md_document
---

```{r setup, include=TRUE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(e1071)
library(ISLR)
library(dplyr)
attach(OJ)
```

###8.a
```{r 8-a}
set.seed(1970)
train = sample(nrow(OJ), 800)
OJ = mutate(OJ, StoreID = as.factor(StoreID), SpecialCH = as.factor(SpecialCH), SpecialMM = as.factor(SpecialMM), STORE = as.factor(STORE))
```

###8.b
```{r 8-b}
svmfit = svm(Purchase~.-WeekofPurchase, data = OJ[train,], kernel = "linear", cost = 0.01, scale = TRUE)
summary(svmfit)
```

A Support Vector Classifier was fitted to the training data and it used 433 observations as support vectors. This number represents over 50% of all training observations.

###8.c
```{r 8-c}
x = table(pred = svmfit$fitted, truth = OJ$Purchase[train])
print(paste("The training error for SVC =", round(sum(x[row(x)!=col(x)])/sum(x),4)))

x = table(pred = predict(svmfit, OJ[-train,]), truth = OJ$Purchase[-train])
print(paste("The test error SVC =", round(sum(x[row(x)!=col(x)])/sum(x),4)))
```

###8.d
```{r 8-d}
tune.out = tune(svm, Purchase~.-WeekofPurchase, data = OJ[train,], kernel = "linear", scale = TRUE,
                ranges = list(cost = c(0.01, 0.1, 1, 2, 5, 10)))
print(paste("The optimal cost for the SVC =", tune.out$best.model$cost))
```

###8.e
```{r 8-e}
x = table(pred = tune.out$best.model$fitted, truth = OJ$Purchase[train])
print(paste("The training error for the optimal SVC =", round(sum(x[row(x)!=col(x)])/sum(x),4)))

x = table(pred = predict(tune.out$best.model, OJ[-train,]), truth = OJ$Purchase[-train])
print(paste("The test error for the optimal SVC =", round(sum(x[row(x)!=col(x)])/sum(x),4)))
```

###8.f
```{r 8-f}
svmfit = svm(Purchase~.-WeekofPurchase, data = OJ[train,], kernel = "radial", cost = 0.01, scale = TRUE)

x = table(pred = svmfit$fitted, truth = OJ$Purchase[train])
print(paste("The training error for SVM with radial kernel =", round(sum(x[row(x)!=col(x)])/sum(x),4)))
x = table(pred = predict(svmfit, OJ[-train,]), truth = OJ$Purchase[-train])
print(paste("The test error for SVM with radial kernel =", round(sum(x[row(x)!=col(x)])/sum(x),4)))

tune.out = tune(svm, Purchase~.-WeekofPurchase, data = OJ[train,], kernel = "radial",
                ranges = list(cost = c(0.01, 0.1, 1, 2, 5, 10)))
print(paste("The optimal cost for SVM with radial kernel =", tune.out$best.model$cost))

x = table(pred = tune.out$best.model$fitted, truth = OJ$Purchase[train])
print(paste("The training error for the optimal SVM with radial kernel =", round(sum(x[row(x)!=col(x)])/sum(x),4)))
x = table(pred = predict(tune.out$best.model, OJ[-train,]), truth = OJ$Purchase[-train])
print(paste("The test error for the optimal SVM with radial kernel =", round(sum(x[row(x)!=col(x)])/sum(x),4)))
```

###8.g
```{r 8-g}
svmfit = svm(Purchase~.-WeekofPurchase, data = OJ[train,], kernel = "polynomial", cost = 0.01, degree = 2, scale = TRUE)

x = table(pred = svmfit$fitted, truth = OJ$Purchase[train])
print(paste("The training error for SVM with polynomial kernel =", round(sum(x[row(x)!=col(x)])/sum(x),4)))
x = table(pred = predict(svmfit, OJ[-train,]), truth = OJ$Purchase[-train])
print(paste("The test error for polynomial kernel fit =", round(sum(x[row(x)!=col(x)])/sum(x),4)))

tune.out = tune(svm, Purchase~.-WeekofPurchase, data = OJ[train,], kernel = "polynomial", degree = 2,
                ranges = list(cost = c(0.01, 0.1, 1, 2, 5, 10)))
print(paste("The optimal cost for SVM with polynomial kernel =", tune.out$best.model$cost))

x = table(pred = tune.out$best.model$fitted, truth = OJ$Purchase[train])
print(paste("The training error for the optimal SVM with polynomial kernel =", round(sum(x[row(x)!=col(x)])/sum(x),4)))
x = table(pred = predict(tune.out$best.model, OJ[-train,]), truth = OJ$Purchase[-train])
print(paste("The test error for the optimal SVM with polynomial kernel =", round(sum(x[row(x)!=col(x)])/sum(x),4)))
```

###8.h
Overall, the SVC with cost=0.01 gives the best result with a test error rate of 17.41%.