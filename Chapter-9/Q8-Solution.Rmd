---
title: "Q8-Solution"
author: "Sanjeev Gadre"
date: "January 10, 2019"
output: md_document
---

```{r setup, include=TRUE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(e1071)
library(ISLR)
library(dplyr)
attach(OJ)
```

###8.a
```{r 8-a}
set.seed(1970)
train = sample(nrow(OJ), 800)
```

###8.b
```{r 8-b}
svmfit = svm(Purchase~., data = OJ[train,], kernel = "linear", cost = 0.01)
summary(svmfit)
```

A Support Vector Classifier was fitted to the training data and it used 431 observations as support vectors. This number represents over 50% of all training observations.

###8.c
```{r 8-c}
x = table(pred = svmfit$fitted, truth = OJ$Purchase[train])
print(paste("The training error for SVC =", round(sum(x[row(x)!=col(x)])/sum(x),4)))

x = table(pred = predict(svmfit, OJ[-train,]), truth = OJ$Purchase[-train])
print(paste("The test error SVC =", round(sum(x[row(x)!=col(x)])/sum(x),4)))
```

###8.d
```{r 8-d}
tune.out = tune(svm, Purchase~., data = OJ[train,], kernel = "linear",
                ranges = list(cost = c(0.01, 0.1, 1, 2, 5, 10)))
print(paste("The optimal cost for the SVC =", tune.out$best.model$cost))
```

###8.e
```{r 8-e}
x = table(pred = tune.out$best.model$fitted, truth = OJ$Purchase[train])
print(paste("The training error for the optimal SVC =", round(sum(x[row(x)!=col(x)])/sum(x),4)))

x = table(pred = predict(tune.out$best.model, OJ[-train,]), truth = OJ$Purchase[-train])
print(paste("The test error for the optimal SVC =", round(sum(x[row(x)!=col(x)])/sum(x),4)))
```

###8.f
```{r 8-f}
svmfit = svm(Purchase~., data = OJ[train,], kernel = "radial", cost = 0.01)

x = table(pred = svmfit$fitted, truth = OJ$Purchase[train])
print(paste("The training error for SVM with radial kernel =", round(sum(x[row(x)!=col(x)])/sum(x),4)))
x = table(pred = predict(svmfit, OJ[-train,]), truth = OJ$Purchase[-train])
print(paste("The test error for SVM with radial kernel =", round(sum(x[row(x)!=col(x)])/sum(x),4)))

tune.out = tune(svm, Purchase~., data = OJ[train,], kernel = "radial",
                ranges = list(cost = c(0.01, 0.1, 1, 2, 5, 10)))
print(paste("The optimal cost for SVM with radial kernel =", tune.out$best.model$cost))

x = table(pred = tune.out$best.model$fitted, truth = OJ$Purchase[train])
print(paste("The training error for the optimal SVM with radial kernel =", round(sum(x[row(x)!=col(x)])/sum(x),4)))
x = table(pred = predict(tune.out$best.model, OJ[-train,]), truth = OJ$Purchase[-train])
print(paste("The test error for the optimal SVM with radial kernel =", round(sum(x[row(x)!=col(x)])/sum(x),4)))
```

###8.g
```{r 8-g}
svmfit = svm(Purchase~., data = OJ[train,], kernel = "polynomial", cost = 0.01, degree = 2)

x = table(pred = svmfit$fitted, truth = OJ$Purchase[train])
print(paste("The training error for SVM with polynomial kernel =", round(sum(x[row(x)!=col(x)])/sum(x),4)))
x = table(pred = predict(svmfit, OJ[-train,]), truth = OJ$Purchase[-train])
print(paste("The test error for polynomial kernel fit =", round(sum(x[row(x)!=col(x)])/sum(x),4)))

tune.out = tune(svm, Purchase~., data = OJ[train,], kernel = "polynomial", degree = 2,
                ranges = list(cost = c(0.01, 0.1, 1, 2, 5, 10)))
print(paste("The optimal cost for SVM with polynomial kernel =", tune.out$best.model$cost))

x = table(pred = tune.out$best.model$fitted, truth = OJ$Purchase[train])
print(paste("The training error for the optimal SVM with polynomial kernel =", round(sum(x[row(x)!=col(x)])/sum(x),4)))
x = table(pred = predict(tune.out$best.model, OJ[-train,]), truth = OJ$Purchase[-train])
print(paste("The test error for the optimal SVM with polynomial kernel =", round(sum(x[row(x)!=col(x)])/sum(x),4)))
```

###8.h
Overall, the SVM with a polynomial kernel with degree=2 and cost=10 gives the best result with a test error rate of 18.52%. 

However, it must be noted that the SVM with radial kernel with gamma=0.06 (default=1/ncol(OJ)) and cost=2 gives a comparable test error rate of 18.89% and the SVC with cost=0.1 does not do too poorly with a test error rate of 19.63%. 