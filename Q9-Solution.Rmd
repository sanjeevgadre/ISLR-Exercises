---
title: "Q9-Solution"
author: "Sanjeev Gadre"
date: "November 11, 2018"
output: md_document
---

```{r setup, include=TRUE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(dplyr)
library(boot) #used for the cv.glm()
library(splines)
library(ggplot2)
data("Boston")
```

###Functions used in solutions below
```{r functions}
poly.fit.9b = function(d){
  fit = lm(nox~poly(dis,d), data = Boston)
  preds = predict(fit, newdata = Boston, type = "response")
}

poly.fit.9c = function(d){
  fit = glm(nox~poly(dis,d), data = Boston)
  cv.error = cv.glm(Boston, fit, K = 5)$delta[2] #K=5, choosing the number of folds
}

poly.fit.9e = function(d){
  fit = lm(nox~bs(dis, df=d), data = Boston)
  preds = predict(fit, newdata = Boston, type = "response")
}

poly.fit.9f = function(d){
  fit = glm(nox~bs(dis,d), data = Boston)
  cv.error = cv.glm(Boston, fit, K = 5)$delta[2]
}
```

###9.a
```{r 9-a}
fit = lm(nox~poly(dis, 3), data = Boston)
summary(fit)
preds = predict(fit, newdata = Boston, type = "response")
Boston.9a = Boston %>% mutate(predicted.nox = preds)

Boston.9a %>% ggplot(aes(dis, nox))+geom_point(position = "jitter", col = "darkgrey")+ geom_line(aes(dis, predicted.nox), col = "blue", size = 1)
```

The summary of the cubic polynomial regression to predict nox using dis clearly shows that the cubic fit provides a good explanation between nox and dis.

###9.b
```{r 9-b}
preds.multi = sapply(1:10, poly.fit.9b)
Boston.9b = Boston %>% mutate(predicted.nox.1 = preds.multi[,1], predicted.nox.2 = preds.multi[,2],
                           predicted.nox.3 = preds.multi[,3], predicted.nox.4 = preds.multi[,4],
                           predicted.nox.5 = preds.multi[,5], predicted.nox.6 = preds.multi[,6],
                           predicted.nox.7 = preds.multi[,7], predicted.nox.8 = preds.multi[,8],
                           predicted.nox.9 = preds.multi[,9], predicted.nox.10 = preds.multi[,10])
Boston.9b %>% ggplot(aes(dis,nox))+ geom_point(col = "darkgrey", position = "jitter")+
  geom_line(aes(dis, predicted.nox.1))+
  geom_line(aes(dis, predicted.nox.2))+
  geom_line(aes(dis, predicted.nox.3))+
  geom_line(aes(dis, predicted.nox.4))+
  geom_line(aes(dis, predicted.nox.5))+
  geom_line(aes(dis, predicted.nox.6))+
  geom_line(aes(dis, predicted.nox.7))+
  geom_line(aes(dis, predicted.nox.8))+
  geom_line(aes(dis, predicted.nox.9))+
  geom_line(aes(dis, predicted.nox.10))

res.df = data.frame(res.1 = (Boston.9b$nox-Boston.9b$predicted.nox.1)^2,
                    res.2 = (Boston.9b$nox-Boston.9b$predicted.nox.2)^2,
                    res.3 = (Boston.9b$nox-Boston.9b$predicted.nox.3)^2,
                    res.4 = (Boston.9b$nox-Boston.9b$predicted.nox.4)^2,
                    res.5 = (Boston.9b$nox-Boston.9b$predicted.nox.5)^2,
                    res.6 = (Boston.9b$nox-Boston.9b$predicted.nox.6)^2,
                    res.7 = (Boston.9b$nox-Boston.9b$predicted.nox.7)^2,
                    res.8 = (Boston.9b$nox-Boston.9b$predicted.nox.8)^2,
                    res.9 = (Boston.9b$nox-Boston.9b$predicted.nox.9)^2,
                    res.10 = (Boston.9b$nox-Boston.9b$predicted.nox.10)^2)
res.df = colSums(res.df)
res.df
```

###9.c
```{r 9-c}
set.seed(1)
cv.error = sapply(1:10, poly.fit.9c)
which.min(cv.error)
```

Using the cross validated test set mean square error we conclude that a polynomial of the 4th degree is the most appropriate polynomial regression fit to predict nox using dis as the predictor variable.

###9.d
```{r 9-d}

fit = lm(nox~bs(dis, df = 4), data = Boston)
summary(fit)
attr(bs(Boston$dis, df = 4), "knots")

data.frame(dis=Boston$dis, nox=Boston$nox, predicted.nox=fit$fitted.values) %>%
  ggplot(aes(dis,nox))+ geom_point(col="dark grey", position="jitter")+ 
  geom_line(aes(dis, predicted.nox), col = "blue")
```

We decided on the location of the knots by specifying the degrees of freedom desired in the bs() function. In this case, this amounted to specifying the degress of freedom as 4. This produces a cubic spline regression fit with one knot placed at the 50th percentile of the range of dis.

###9.e
```{r 9-e}
preds.multi = sapply(3:12, poly.fit.9e)

Boston.9e = Boston %>% mutate(predicted.nox.3 = preds.multi[,1], predicted.nox.4 = preds.multi[,2],
                              predicted.nox.5 = preds.multi[,1], predicted.nox.6 = preds.multi[,2],
                              predicted.nox.7 = preds.multi[,1], predicted.nox.8 = preds.multi[,2],
                              predicted.nox.9 = preds.multi[,1], predicted.nox.10 = preds.multi[,2],
                              predicted.nox.11 = preds.multi[,1], predicted.nox.12 = preds.multi[,2])

Boston.9e %>% ggplot(aes(dis,nox))+ geom_point(position = "jitter", col = "dark grey")+
  geom_line(aes(dis, predicted.nox.3))+ geom_line(aes(dis, predicted.nox.4))+
  geom_line(aes(dis, predicted.nox.5))+ geom_line(aes(dis, predicted.nox.6))+
  geom_line(aes(dis, predicted.nox.7))+ geom_line(aes(dis, predicted.nox.8))+
  geom_line(aes(dis, predicted.nox.9))+ geom_line(aes(dis, predicted.nox.10))+
  geom_line(aes(dis, predicted.nox.11))+ geom_line(aes(dis, predicted.nox.12))

res.df = data.frame(res.3 = (preds.multi[,1]-Boston$nox)^2, res.4 = (preds.multi[,2]-Boston$nox)^2,
                    res.5 = (preds.multi[,3]-Boston$nox)^2, res.6 = (preds.multi[,4]-Boston$nox)^2,
                    res.7 = (preds.multi[,5]-Boston$nox)^2, res.8 = (preds.multi[,6]-Boston$nox)^2,
                    res.9 = (preds.multi[,7]-Boston$nox)^2, res.10 = (preds.multi[,8]-Boston$nox)^2,
                    res.11 = (preds.multi[,9]-Boston$nox)^2, res.12= (preds.multi[,10]-Boston$nox)^2)           %>% colSums(.)
res.df
plot(3:12, res.df, type = "l", lwd = "2", xlab = "Degrees of Freedom", ylab = "Sum of squared errors")
```

We fitted regression splines for degrees of freedom varying from 3 through 12. We then caclulated the sum of square errors for each degree. The results show that sum of square errors reduces almost monotonically as the degrees of freedom increases.

###9.f
```{r 9-f}
set.seed(1)
cv.error = sapply(3:12, poly.fit.9f)
which.min(cv.error)
```

Cross validation recommends that we choose a regression spline with 8 degrees of freedom for the lowest test-set mean of squared errors.
