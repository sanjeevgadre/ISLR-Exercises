---
title: "Chap3Exercises"
author: "Sanjeev Gadre"
date: "August 1, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Including the necessary libraries

```{r including-libraries, message=FALSE, echo=TRUE}
library(ISLR)
library(car)
library(tidyverse)
```

## Problem 13

#13.a
```{r, echo=TRUE}
set.seed(1)
x = rnorm(100)
```

#13.b
```{r, echo=TRUE}
eps= rnorm(100,0,sqrt(0.25))
```

#13.c
```{r}
y = -1+0.5*x+eps
```
The length of vector y is 100 and the values of beta_0 and beta_1 in this linear model are respectively -1 and 0.5.

#13.d
```{r}
data.frame(y=y, x=x) %>% ggplot(aes(x,y))+ geom_point()
```

The scatter plot seems to indicate that y increases monotonically with x, something we expect because of the model definition. However the noise term (eps) means that y's are not on a straight line but scattered about.

#13.e
```{r}
linear_fit_13e = lm(y~x)
summary_13e = summary(linear_fit_13e)
beta_hat_0_13e = summary_13e$coefficients[1]
beta_hat_1_13e = summary_13e$coefficients[2]
```

The beta_hat_0 value of
```{r, echo=FALSE}
print(beta_hat_0_13e, digits=3)
```

is very close to beta_0 = -1 and similarly the beta_hat_1 value of 
```{r, echo=FALSE}
print(beta_hat_1_13e, digits=3)
```

is very close to beta_1 = 0.5. The differences are attributable to the noise term eps.

#13.f
```{r}
data.frame(y=y, x=x) %>% ggplot(aes(x,y))+ geom_point()+ geom_abline(slope=beta_hat_1_13e, intercept=beta_hat_0_13e, col="RED", show.legend=TRUE)
```

#13.g
```{r}
linear_fit_13g = lm(y~x+I(x^2))
summary_13g = summary(linear_fit_13g)
R_sq_13g = summary_13g$r.squared

R_sq_13e = summary_13e$r.squared

#R-squared for fit with only linear term
R_sq_13e 
#R-squared for fit with linear and quadratic term
R_sq_13g 
```
The R-squared value with the quadratic term is slightly better than the R-squared value with only a linear term. This is a classic example of "over-fitting". The noise term eps means that y does not have a strict linear relationship with x and therefore the quadratic term helps create a more "wiggly" line that better fits the training data. However it is fully expected that with test data, the linear-term-only model would perform much better.

#13.h
```{r}
eps_less = rnorm(100,0,sqrt(0.025))
y = -1+0.5*x+eps_less

data.frame(y=y, x=x) %>% ggplot(aes(x,y))+ geom_point()

linear_fit_13h = lm(y~x)
summary_13h = summary(linear_fit_13h)
beta_hat_0_13h = summary_13h$coefficients[1]
beta_hat_1_13h = summary_13h$coefficients[2]
R_sq_13h = summary_13h$r.squared

data.frame(y=y, x=x) %>% ggplot(aes(x,y))+ geom_point()+ geom_abline(slope=beta_hat_1_13h, intercept=beta_hat_0_13h, col="Red", show.legend=TRUE)
```

Some commentary 

#13.i
```{r}
eps_more = rnorm(100,0,sqrt(2.5))
y = -1+0.5*x+eps_more

data.frame(y=y, x=x) %>% ggplot(aes(x,y))+ geom_point()

linear_fit_13i = lm(y~x)
summary_13i = summary(linear_fit_13i)
beta_hat_0_13i = summary_13i$coefficients[1]
beta_hat_1_13i = summary_13i$coefficients[2]
R_sq_13i = summary_13i$r.squared

data.frame(y=y, x=x) %>% ggplot(aes(x,y))+ geom_point()+ geom_abline(slope=beta_hat_1_13i, intercept=beta_hat_0_13i, col="Red", show.legend=TRUE)
```

